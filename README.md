# House Sale Price Prediction (Regression Project)
Predicting housing sale prices using machine learning and Exploratory Data Analysis (EDA) in R.

## üìå Project Overview
This project focuses on predicting house sale prices using the **Ames Housing Dataset**.  
The goal was to analyze the data, engineer meaningful features, compare multiple models, and select the best-performing approach based on Kaggle leaderboard RMSE.

The final chosen model was a **Gradient Boosting Machine (GBM)**, which performed best due to its ability to capture nonlinear relationships and interactions between variables.

---

## üßπ Data Cleaning & Feature Engineering
To prepare the dataset for modeling, the following steps were applied:

- Median imputation for missing numeric values
- Mode imputation for missing categorical values
- Removal of unused factor levels & single-level categorical variables
- Creation of engineered features:
  - `TotalSF` = total finished square footage
  - `Age` = 2025 - year built
  - `TotalBaths` = full + half + basement baths
  - `PorchSF` = combined outdoor porch features
- Log transformation of the response variable:
  \[
  Y = \log(\text{SalePrice})
  \]

---

## üìä Exploratory Data Analysis (EDA)
Key findings from the EDA:

- SalePrice is **right-skewed**, and log transformation improves distribution shape.
- Strongest correlated predictors:
  - Overall Quality
  - Above-ground living area (`Gr.Liv.Area`)
  - Garage capacity (`Garage.Cars`)
- Living area and sale price show **nonlinear patterns** with diminishing returns.
- Majority of missing data is limited; *Alley* has ~93% missing values.

---

## ü§ñ Models Trained & Compared
| Model                | Result | Notes |
|----------------------|--------|-------|
| LASSO Regression     | ‚ùå Higher RMSE | Too linear, misses nonlinear trends |
| XGBoost              | ‚ö†Ô∏è Underperformed | Limited tuning & complexity issues |
| Random Forest        | ‚ö†Ô∏è Decent | Good interaction capture, lower accuracy |
| **GBM (Final Model)**| ‚úÖ Best Score | Best RMSE on Kaggle, strong nonlinear modeling |

Final Model: **Gradient Boosting Machine**

\[
\hat{f}(x) = \sum_{m=1}^{M} \nu T_m(x)
\]

Where:
- \( M = \) number of trees (10,000)
- \( \nu = \) learning rate (0.005)
- \( T_m(x) = \) m-th regression tree

---

## üöÄ Kaggle Submission
- Metric: **RMSE (lower is better)**
- Final GBM Score: **6.3 (best performing model)**

Predictions were generated by:
```r
pred <- exp(predict(gbm_model, newdata = test_clean, n.trees = best_iter))
